{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9be6a381",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Импортируем все необходимое\n",
    "import pandas as pd\n",
    "import torch \n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset, dataloader\n",
    "import gymnasium as gym\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1b745c",
   "metadata": {},
   "source": [
    "Вспомогательные функции для SimRCRL-лосса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcfc5e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indicator(i,j):\n",
    "    if i==j:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "    \n",
    "def denominator(i, batch_ah, batch_p, tau):\n",
    "    value=0\n",
    "    for j in range(len(batch_ah)):\n",
    "        dot1=torch.exp(torch.dot(batch_ah[i], batch_ah[j])/tau)\n",
    "        dot2=torch.exp(torch.dot(batch_ah[i], batch_p[j])/tau)\n",
    "        \n",
    "        value+=indicator(i, j)*(dot1 + dot2)\n",
    "    return value\n",
    "\n",
    "\n",
    "def RCRL_loss(batch_ah, batch_p):\n",
    "    tau=.1\n",
    "    res=0\n",
    "    \n",
    "    for i in range(len(batch_ah)):\n",
    "        dot=torch.exp(torch.dot(batch_ah[i], batch_p[i])/tau)\n",
    "        value=denominator(i, batch_ah, batch_p, tau)\n",
    "        res+= (-1)*torch.log(dot/value)\n",
    "        \n",
    "    return res\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf67369",
   "metadata": {},
   "source": [
    "Реализация bucket-sampling для построения эмбеддингов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a92078e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_separated_batch(batch, g_list, L):\n",
    "    bk_list=np.linspace(min(g_list), max(g_list), L)\n",
    "    \n",
    "    separated_batch=defaultdict(list)\n",
    "    positive_batch=[]\n",
    "    \n",
    "    \n",
    "    for j in range(len(g_list)):\n",
    "        for i in range(len(bk_list)-1):\n",
    "            if (g_list[j]>=bk_list[i] and g_list[j]<=bk_list[i+1]):\n",
    "                separated_batch[bk_list[i]].append(batch[j])\n",
    "                \n",
    "    for j in range(len(g_list)):\n",
    "        for i in range(len(bk_list)-1):\n",
    "            if (g_list[j]>=bk_list[i] and g_list[j]<=bk_list[i+1]):\n",
    "                positive_batch.append(random.choice(separated_batch[bk_list[i]]))\n",
    "    \n",
    "    return positive_batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e7179b",
   "metadata": {},
   "source": [
    "Класс для построения contrastive embeddings и positive/negative tokens sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd310e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class contrastive_embeddings(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_size, sequence_length, state_dim, act_dim, buckets_num):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.sequence_length=sequence_length\n",
    "        self.state_dim=state_dim\n",
    "        self.act_dim=act_dim\n",
    "        self.buckets_num=buckets_num\n",
    "        self.embedding_size=embedding_size\n",
    "        \n",
    "        self.states_embed=nn.Linear(self.state_dim, self.embedding_size, bias=True)\n",
    "        self.actions_embed=nn.Linear(self.act_dim, self.embedding_size, bias=True)\n",
    "        self.rewards_embed=nn.Linear(1, self.embedding_size, bias=True)\n",
    "        self.timesteps_embed=nn.Embedding(1000, self.embedding_size)\n",
    "        \n",
    "    def forward(self, states, actions, rewards, return_to_go=0,  \n",
    "                attentions_mask=0, train=True):\n",
    "        if train:\n",
    "            states_embeddings=self.states_embed(states)\n",
    "            actions_embeddings=self.actions_embed(actions)\n",
    "            rewards_embeddings=self.rewards_embed(rewards.reshape((10,1)))\n",
    "            #timesteps_embeddings=self.timesteps_embed(states)\n",
    "            \n",
    "            z_ah_embeddings=np.multiply(states_embeddings.detach(), actions_embeddings.detach())\n",
    "            positives=get_separated_batch(z_ah_embeddings, rewards, self.buckets_num)\n",
    "            positives=torch.stack(positives)\n",
    "            positives=torch.tensor(positives, requires_grad=True)\n",
    "            z_ah_embeddings=torch.tensor(z_ah_embeddings, requires_grad=True)\n",
    "            \n",
    "            \n",
    "            return states_embeddings, actions_embeddings, z_ah_embeddings, positives\n",
    "        \n",
    "        \n",
    "        else:\n",
    "            states_embeddings=self.states_embed(states)\n",
    "            actions_embeddings=self.actions_embed(actions)\n",
    "            #timesteps_embeddings=self.timesteps_embed(states)\n",
    "            \n",
    "            return states_embeddings, actions_embeddings\n",
    "            \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8f15cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data, transforms=None):\n",
    "        self.data=data\n",
    "        self.transforms=transoforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ee7218",
   "metadata": {},
   "source": [
    "Блок для самого Decision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82f3e871",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_heads, ff_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.query_emb=nn.Linear(input_size, hidden_size)\n",
    "        self.key_emb=nn.Linear(input_size, hidden_size)\n",
    "        self.value_emb=nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "        self.attention = torch.nn.MultiheadAttention(hidden_size, num_heads, dropout)\n",
    "        self.ffn = nn.Linear(hidden_size, ff_size, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        query=self.query_emb(x)\n",
    "        key=self.key_emb(x)\n",
    "        value=self.value_emb(x)\n",
    "        \n",
    "        x = self.attention(query, key, value)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff7af2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT_2(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_blocks, input_size, hidden_size, num_heads, ff_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.single_layer=TransformerBlock(input_size, hidden_size, num_heads, ff_size, dropout=0.1)\n",
    "        layers=[self.single_layer for i in range(num_blocks)]\n",
    "        self.layers=nn.ModuleList(layers)\n",
    "        self.model=nn.Sequential(self.layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output=torch.zeros(x.shape)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            output+=layer(x)[0]\n",
    "            \n",
    "            \n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "916e1d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveDT(nn.Module):\n",
    "    \n",
    "    def __init__(self,embedding_size, sequence_length, state_dim, act_dim, buckets_num,\n",
    "                num_blocks, input_size, hidden_size, num_heads, ff_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding_size=embedding_size\n",
    "        self.sequence_length=sequence_length\n",
    "        self.state_dim=state_dim\n",
    "        self.act_dim=act_dim\n",
    "        self.buckets_num=buckets_num\n",
    "        self.num_blocks=num_blocks\n",
    "        self.input_size=input_size\n",
    "        self.hidden_size=hidden_size\n",
    "        self.num_heads=num_heads \n",
    "        self.ff_size=ff_size\n",
    "        self.dropout=dropout\n",
    "        \n",
    "        self.embedding_layer=contrastive_embeddings(self.embedding_size, self.sequence_length, \n",
    "                                                      self.state_dim, self.act_dim, self.buckets_num)\n",
    "        \n",
    "        self.gpt_layer=GPT_2(self.num_blocks, self.embedding_size, self.hidden_size, \n",
    "                             self.num_heads, self.ff_size, self.dropout)\n",
    "        \n",
    "        \n",
    "        \n",
    "        dim=2*self.embedding_size*self.sequence_length\n",
    "        self.output_states=nn.Linear(dim, self.sequence_length*self.state_dim)\n",
    "        self.output_actions=nn.Linear(dim, self.sequence_length*self.act_dim)\n",
    "        self.output_rewards=nn.Linear(dim, self.sequence_length)\n",
    "        \n",
    "    def forward(self, states, actions, rewards, train=True):\n",
    "        embeddings=self.embedding_layer(states, actions, rewards)\n",
    "        states_embeddings=list(embeddings[0].detach().numpy()[::-1])\n",
    "        action_embeddings=list(embeddings[1].detach().numpy()[::-1])\n",
    "        \n",
    "        gpt_input_embeddings=[]\n",
    "        for i in range(len(states_embeddings)*2):\n",
    "            \n",
    "            if i%2==0:\n",
    "                gpt_input_embeddings.append(torch.tensor(states_embeddings.pop()))\n",
    "            else:\n",
    "                gpt_input_embeddings.append(torch.tensor(action_embeddings.pop()))\n",
    "                \n",
    "        gpt_input_embeddings=torch.stack(gpt_input_embeddings)\n",
    "        \n",
    "        out=self.gpt_layer(gpt_input_embeddings)\n",
    "        \n",
    "        out=out.reshape((2*self.embedding_size*self.sequence_length,))\n",
    "        \n",
    "        out_states=self.output_states(out).reshape((self.sequence_length,self.state_dim))\n",
    "        out_actions=self.output_actions(out).reshape((self.sequence_length,self.act_dim))\n",
    "        out_rewards=self.output_rewards(out).reshape((self.sequence_length))\n",
    "        \n",
    "        out=list(zip(out_states, out_actions, out_rewards))\n",
    "        #out=torch.stack(out)\n",
    "        \n",
    "        #Output - тройки (s, a, g), тензор состояний, тензор действий, тензор rewards и positive/negative\n",
    "\n",
    "        \n",
    "        return out, out_states, out_actions, out_rewards, embeddings[2], embeddings[3]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e766e03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ContrastiveDT(embedding_size=128, sequence_length=10, state_dim=11, act_dim=3, buckets_num=5,\n",
    "                num_blocks=12, input_size=128, hidden_size=128, num_heads=8, ff_size=128, dropout=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec26d856",
   "metadata": {},
   "source": [
    "Блок подготовки данных для экспериментов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "88cf67dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sample(path, seed_value, action_dim=0):\n",
    "    env=gym.make(path)\n",
    "    observation, info = env.reset(seed=seed_value)\n",
    "    \n",
    "    action_num = env.action_space.sample()  # this is where you would insert your policy\n",
    "    observation, reward, terminated, truncated, info = env.step(action_num)\n",
    "    action=torch.zeros((action_dim))\n",
    "    action[action_num]=1\n",
    "    \n",
    "    \n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "    env.close()\n",
    "    \n",
    "    observation = observation.flatten()\n",
    "    \n",
    "    return observation, action, reward\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "859d50a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_dataset(path, length, action_dim=0):\n",
    "    states_data=[]\n",
    "    actions_data=[]\n",
    "    rewards_data=[]\n",
    "    for i in range(length):\n",
    "        observation, action, reward=prepare_sample(path, i, action_dim)\n",
    "        states_data.append(torch.tensor(observation, dtype=torch.float64, requires_grad=True))\n",
    "        actions_data.append(torch.tensor(action, requires_grad=True))\n",
    "        rewards_data.append(torch.tensor(reward, requires_grad=True))\n",
    "        \n",
    "    return torch.stack(states_data), torch.stack(actions_data), torch.stack(rewards_data)\n",
    "        \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
